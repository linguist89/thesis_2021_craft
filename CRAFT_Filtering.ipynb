{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoNLL_Filtering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8INAuX5WxXnF",
        "outputId": "63245a03-a401-4d9b-e45a-73910c21c841"
      },
      "source": [
        "!git clone https://github.com/mandarjoshi90/coref.git\n",
        "%cd coref"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coref'...\n",
            "remote: Enumerating objects: 734, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 734 (delta 2), reused 0 (delta 0), pack-reused 728\u001b[K\n",
            "Receiving objects: 100% (734/734), 4.17 MiB | 8.78 MiB/s, done.\n",
            "Resolving deltas: 100% (441/441), done.\n",
            "/content/coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQA3jx99r4ha"
      },
      "source": [
        "train_split = 0.75\n",
        "dev_split = 0.15\n",
        "test_split = 0.1\n",
        "seg_count = 10"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYLrL5F_yJ7v",
        "outputId": "abdc64ca-5ec3-463c-f494-2917bb9c863c"
      },
      "source": [
        "# Installing dependencies (takes about 5 minutes)\n",
        "import os\n",
        "! sed 's/MarkupSafe==1.0/MarkupSafe==1.1.1/; s/scikit-learn==0.19.1/scikit-learn==0.21/; s/scipy==1.0.0/scipy==1.6.2/' < requirements.txt > tmp\n",
        "! mv tmp requirements.txt\n",
        "\n",
        "! sed 's/.D.GLIBCXX.USE.CXX11.ABI.0//' < setup_all.sh  > tmp\n",
        "! mv tmp setup_all.sh \n",
        "! chmod u+x setup_all.sh \n",
        "\n",
        "model_name = \"bert_base\"\n",
        "os.environ['data_dir'] = \".\"\n",
        "os.environ['CHOSEN_MODEL'] = model_name\n",
        "%tensorflow_version 2.x\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -r requirements.txt --log install-log.txt -q\n",
        "! ./setup_all.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 3.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6MB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 16.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 34.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 27.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 36.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 39.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 39.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 10.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.3MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 39.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 21.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 430kB 36.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 47.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 39.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 36.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.7MB 39.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 27.4MB 134kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 31.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 30.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 377.1MB 36kB/s \n",
            "\u001b[K     |████████████████████████████████| 748.9MB 17kB/s \n",
            "\u001b[K     |████████████████████████████████| 8.8MB 25.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 327kB 46.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1MB 24.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 3.1MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for msgpack-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for psycopg2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kapre 0.3.5 requires tensorflow>=2.0.0, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 1.0.0 has requirement absl-py<0.13,>=0.9, but you'll have absl-py 0.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyasn1-modules 0.2.8 has requirement pyasn1<0.5.0,>=0.4.6, but you'll have pyasn1 0.4.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement python-dateutil>=2.7.3, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.17.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: googleapis-common-protos 1.53.0 has requirement protobuf>=3.12.0, but you'll have protobuf 3.9.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement astor~=0.8.1, but you'll have astor 0.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement protobuf>=3.12.0, but you'll have protobuf 3.9.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flask 1.1.4 has requirement Jinja2<3.0,>=2.10.1, but you'll have jinja2 2.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: flask 1.1.4 has requirement Werkzeug<2.0,>=0.15, but you'll have werkzeug 0.14.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement python-dateutil>=2.8.0, but you'll have python-dateutil 2.6.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: bokeh 2.3.2 has requirement pillow>=7.1.0, but you'll have pillow 6.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF6pIeVGxkFr",
        "outputId": "b8ea2acc-bc7e-41f3-d37f-76c346f35f67"
      },
      "source": [
        "# Create new train_dev_test and line splits\n",
        "# Default is 751510 and 11lines\n",
        "# Skip this cell if you already have sufficient train,dev,test conll files\n",
        "!unzip craft_conll.zip\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def remove_discontinuous(filename):\n",
        "    with open(filename) as f:\n",
        "        text = f.readlines()\n",
        "    text = [t.split(\"\\t\") for t in text if t != \"\\n\"]\n",
        "    doc_num = text[1][0]\n",
        "    line_len = len(text[1])\n",
        "    temp_line = []\n",
        "    edited_text = []\n",
        "    for t in text:\n",
        "        temp_coref = t[-1].split(\"|\")\n",
        "        edited_coref = []\n",
        "        for e in temp_coref:\n",
        "            if e.islower():\n",
        "                edited_coref.append(\"-\")\n",
        "            else:\n",
        "                edited_coref.append(e)\n",
        "        if len(t) == line_len:\n",
        "            if len(edited_coref) > 1:\n",
        "                edited_coref = [e for e in edited_coref if e != \"-\"]\n",
        "            if edited_coref:\n",
        "                edited_text.append('\\t'.join(t[:10]).replace(doc_num, \"nw/{}\".format(doc_num)) + \"\\t*\\t*\\t*\\t\" + '|'.join(edited_coref) + \"\\n\") \n",
        "            else:\n",
        "                edited_text.append('\\t'.join(t[:10]).replace(doc_num, \"nw/{}\".format(doc_num)) + \"\\t*\\t*\\t*\\t\" + '-' + \"\\n\") \n",
        "        else:\n",
        "            edited_text.append(t[0].replace(doc_num, \"nw/{}\".format(doc_num)) + \"\\n\")\n",
        "    edited_text = [l.replace(\"\\n\\n\", \"\\n\") for l in edited_text]\n",
        "    edited_text.append(\"#end document\\n\")\n",
        "    return edited_text \n",
        "\n",
        "files = glob.glob(\"conll_coref/*.conll\")\n",
        "train_percentage = int(len(files)*train_split)\n",
        "dev_percentage = int(len(files)*dev_split)\n",
        "test_percentage = int(len(files)*test_split)\n",
        "train_text = [remove_discontinuous(t) for t in files[:train_percentage]]\n",
        "dev_text = [remove_discontinuous(t) for t in files[train_percentage:train_percentage+dev_percentage]]\n",
        "test_text = [remove_discontinuous(t) for t in files[train_percentage+dev_percentage:]]\n",
        "\n",
        "def add_begin_end(temp_text,\n",
        "                  seg_count=10):\n",
        "    insert_begin = temp_text[0]\n",
        "    insert_end = temp_text[-1]\n",
        "    temp_len = len(temp_text)\n",
        "    increase = 0\n",
        "    count = 0\n",
        "    part_num = 1\n",
        "    segment_counter = 0\n",
        "    for i in range(0,temp_len):\n",
        "        if count < temp_len-2:\n",
        "            count = i + increase\n",
        "            first_split = temp_text[count+1].split(\"\\t\")\n",
        "            second_split = temp_text[count].split(\"\\t\")\n",
        "            if len(first_split) == 14 and len(second_split) == 14:\n",
        "                if int(first_split[2]) < int(second_split[2]):\n",
        "                    segment_counter += 1\n",
        "                    if segment_counter == seg_count:\n",
        "                        temp_text.insert(count+1, insert_end)\n",
        "                        if part_num < 10:\n",
        "                            temp_text.insert(count+2, insert_begin.replace(\"part 000\", \"part 00{}\".format(part_num)))\n",
        "                        elif part_num > 9 and part_num < 100:\n",
        "                            temp_text.insert(count+2, insert_begin.replace(\"part 000\", \"part 0{}\".format(part_num)))\n",
        "                        else:\n",
        "                            temp_text.insert(count+2, insert_begin.replace(\"part 000\", \"part {}\".format(part_num)))\n",
        "                        part_num += 1\n",
        "                        increase += 2\n",
        "                        segment_counter = 0\n",
        "    return temp_text\n",
        "\n",
        "def join_lists(temp_list):\n",
        "    full_list = []\n",
        "    for element in temp_list:\n",
        "        for line in element:\n",
        "            full_list.append(line)\n",
        "    return full_list\n",
        "\n",
        "def remove_long_links(temp_text):\n",
        "    docs_list = []\n",
        "    temp_doc = []\n",
        "    single_coref = \"\\(\\d+\\)\"\n",
        "    start_coref = \"\\(\\d+$\"\n",
        "    end_coref = \"^d+\\)\"\n",
        "    for line in temp_text:\n",
        "        if line.startswith(\"#begin\"):\n",
        "            temp_doc.append(line)\n",
        "        elif line.startswith(\"#end\"):\n",
        "            temp_doc.append(line)\n",
        "            docs_list.append(temp_doc)\n",
        "            temp_doc = []\n",
        "        else:\n",
        "            temp_doc.append(line)\n",
        "    \n",
        "    \n",
        "    edited_docs = []\n",
        "    for doc in docs_list:\n",
        "        edited_doc = []\n",
        "        all_doc_corefs = []\n",
        "        for line in doc:\n",
        "            line_split = line.split(\"\\t\")\n",
        "            if len(line_split) == 14:\n",
        "                if line_split[-1] != \"-\\n\":\n",
        "                    line_coref = line_split[-1].strip(\"\\n\")\n",
        "                    if \"|\" in line_coref:\n",
        "                        for e in line_coref.split(\"|\"):\n",
        "                            all_doc_corefs.append(e)\n",
        "                    else:\n",
        "                        all_doc_corefs.append(line_coref)\n",
        "    \n",
        "        for line in doc:\n",
        "            if len(line.split(\"\\t\")) == 14:\n",
        "                temp_coref = line.split(\"\\t\")[-1].strip(\"\\n\").split(\"|\")\n",
        "                edited_coref = []\n",
        "                for tc in temp_coref:\n",
        "                    if re.search(single_coref, tc):\n",
        "                        edited_coref.append(tc)\n",
        "                    elif re.search(start_coref, tc):\n",
        "                        find_end = \"{})\".format(tc.split(\"(\")[1])\n",
        "                        if find_end in all_doc_corefs:\n",
        "                            edited_coref.append(tc)\n",
        "                        else:\n",
        "                            edited_coref.append(\"-\")\n",
        "                    elif re.search(end_coref, tc):\n",
        "                        find_start = \"({}\".format(tc.split(\")\"[0]))\n",
        "                        if find_start in all_doc_corefs:\n",
        "                            edited_coref.append(tc)\n",
        "                        else:\n",
        "                            edited_coref.append(\"-\") \n",
        "                    else:\n",
        "                        edited_coref.append(tc)                           \n",
        "                if len(edited_coref) > 1 and \"-\" not in edited_coref:\n",
        "                    edited_doc.append('\\t'.join(line.split(\"\\t\")[:-1]) + \"\\t\" + \"|\".join(edited_coref) + \"\\n\")\n",
        "                elif len(edited_coref) == 1:\n",
        "                    edited_doc.append('\\t'.join(line.split(\"\\t\")[:-1]) + \"\\t\" + edited_coref[0] + \"\\n\")\n",
        "            else:\n",
        "                edited_doc.append(line)\n",
        "        edited_docs.append(edited_doc)  \n",
        "    return edited_docs\n",
        "\n",
        "\n",
        "train_final = []\n",
        "for t in train_text:\n",
        "    temp_list = t\n",
        "    temp_list = add_begin_end(t,seg_count=seg_count)\n",
        "    temp_list = remove_long_links(t)\n",
        "    for line in temp_list:\n",
        "        train_final.append(line)\n",
        "\n",
        "dev_final = []\n",
        "for t in dev_text:\n",
        "    temp_list = t\n",
        "    temp_list = add_begin_end(t, seg_count=seg_count)\n",
        "    temp_list = remove_long_links(t)\n",
        "    for line in temp_list:\n",
        "        dev_final.append(line)\n",
        "\n",
        "test_final = []\n",
        "for t in test_text:\n",
        "    temp_list = t\n",
        "    temp_list = add_begin_end(t,seg_count=seg_count)\n",
        "    temp_list = remove_long_links(t)\n",
        "    for line in temp_list:\n",
        "        test_final.append(line)\n",
        "with open(\"train.english.v4_gold_conll\", \"w\") as f:\n",
        "    f.writelines(join_lists(train_text))\n",
        "with open(\"dev.english.v4_gold_conll\", \"w\") as f:\n",
        "    f.writelines(join_lists(dev_text))\n",
        "with open(\"test.english.v4_gold_conll\", \"w\") as f:\n",
        "    f.writelines(join_lists(test_text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  craft_conll.zip\n",
            "   creating: conll_coref/\n",
            "  inflating: conll_coref/11319941.conll  \n",
            "  inflating: conll_coref/11532192.conll  \n",
            "  inflating: conll_coref/11597317.conll  \n",
            "  inflating: conll_coref/11604102.conll  \n",
            "  inflating: conll_coref/11897010.conll  \n",
            "  inflating: conll_coref/12079497.conll  \n",
            "  inflating: conll_coref/12546709.conll  \n",
            "  inflating: conll_coref/12585968.conll  \n",
            "  inflating: conll_coref/12925238.conll  \n",
            "  inflating: conll_coref/14609438.conll  \n",
            "  inflating: conll_coref/14611657.conll  \n",
            "  inflating: conll_coref/14624252.conll  \n",
            "  inflating: conll_coref/14675480.conll  \n",
            "  inflating: conll_coref/14691534.conll  \n",
            "  inflating: conll_coref/14723793.conll  \n",
            "  inflating: conll_coref/14737183.conll  \n",
            "  inflating: conll_coref/15005800.conll  \n",
            "  inflating: conll_coref/15018652.conll  \n",
            "  inflating: conll_coref/15040800.conll  \n",
            "  inflating: conll_coref/15061865.conll  \n",
            "  inflating: conll_coref/15070402.conll  \n",
            "  inflating: conll_coref/15207008.conll  \n",
            "  inflating: conll_coref/15238161.conll  \n",
            "  inflating: conll_coref/15314655.conll  \n",
            "  inflating: conll_coref/15314659.conll  \n",
            "  inflating: conll_coref/15320950.conll  \n",
            "  inflating: conll_coref/15328533.conll  \n",
            "  inflating: conll_coref/15328538.conll  \n",
            "  inflating: conll_coref/15345036.conll  \n",
            "  inflating: conll_coref/15492776.conll  \n",
            "  inflating: conll_coref/15550985.conll  \n",
            "  inflating: conll_coref/15560850.conll  \n",
            "  inflating: conll_coref/15588329.conll  \n",
            "  inflating: conll_coref/15615595.conll  \n",
            "  inflating: conll_coref/15619330.conll  \n",
            "  inflating: conll_coref/15630473.conll  \n",
            "  inflating: conll_coref/15676071.conll  \n",
            "  inflating: conll_coref/15760270.conll  \n",
            "  inflating: conll_coref/15784609.conll  \n",
            "  inflating: conll_coref/15819996.conll  \n",
            "  inflating: conll_coref/15836427.conll  \n",
            "  inflating: conll_coref/15850489.conll  \n",
            "  inflating: conll_coref/15876356.conll  \n",
            "  inflating: conll_coref/15882093.conll  \n",
            "  inflating: conll_coref/15917436.conll  \n",
            "  inflating: conll_coref/15921521.conll  \n",
            "  inflating: conll_coref/15938754.conll  \n",
            "  inflating: conll_coref/16026622.conll  \n",
            "  inflating: conll_coref/16027110.conll  \n",
            "  inflating: conll_coref/16098226.conll  \n",
            "  inflating: conll_coref/16103912.conll  \n",
            "  inflating: conll_coref/16109169.conll  \n",
            "  inflating: conll_coref/16110338.conll  \n",
            "  inflating: conll_coref/16121255.conll  \n",
            "  inflating: conll_coref/16121256.conll  \n",
            "  inflating: conll_coref/16216087.conll  \n",
            "  inflating: conll_coref/16221973.conll  \n",
            "  inflating: conll_coref/16255782.conll  \n",
            "  inflating: conll_coref/16279840.conll  \n",
            "  inflating: conll_coref/16362077.conll  \n",
            "  inflating: conll_coref/16410827.conll  \n",
            "  inflating: conll_coref/16433929.conll  \n",
            "  inflating: conll_coref/16462940.conll  \n",
            "  inflating: conll_coref/16504143.conll  \n",
            "  inflating: conll_coref/16504174.conll  \n",
            "  inflating: conll_coref/16507151.conll  \n",
            "  inflating: conll_coref/16517939.conll  \n",
            "  inflating: conll_coref/16539743.conll  \n",
            "  inflating: conll_coref/16579849.conll  \n",
            "  inflating: conll_coref/16611361.conll  \n",
            "  inflating: conll_coref/16628246.conll  \n",
            "  inflating: conll_coref/16670015.conll  \n",
            "  inflating: conll_coref/16700629.conll  \n",
            "  inflating: conll_coref/16787536.conll  \n",
            "  inflating: conll_coref/16800892.conll  \n",
            "  inflating: conll_coref/16870721.conll  \n",
            "  inflating: conll_coref/16968134.conll  \n",
            "  inflating: conll_coref/17002498.conll  \n",
            "  inflating: conll_coref/17020410.conll  \n",
            "  inflating: conll_coref/17022820.conll  \n",
            "  inflating: conll_coref/17029558.conll  \n",
            "  inflating: conll_coref/17069463.conll  \n",
            "  inflating: conll_coref/17078885.conll  \n",
            "  inflating: conll_coref/17083276.conll  \n",
            "  inflating: conll_coref/17194222.conll  \n",
            "  inflating: conll_coref/17201918.conll  \n",
            "  inflating: conll_coref/17206865.conll  \n",
            "  inflating: conll_coref/17244351.conll  \n",
            "  inflating: conll_coref/17425782.conll  \n",
            "  inflating: conll_coref/17447844.conll  \n",
            "  inflating: conll_coref/17465682.conll  \n",
            "  inflating: conll_coref/17503968.conll  \n",
            "  inflating: conll_coref/17565376.conll  \n",
            "  inflating: conll_coref/17590087.conll  \n",
            "  inflating: conll_coref/17608565.conll  \n",
            "  inflating: conll_coref/17677002.conll  \n",
            "  inflating: conll_coref/17696610.conll  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaQECFtwx3MO"
      },
      "source": [
        "# Replace code for train, dev, test\n",
        "def word_replace(text,\n",
        "                 search_word,\n",
        "                 replace_word):\n",
        "    edited_text = []\n",
        "    replace_word = \"\\t{}\\t\".format(replace_word)\n",
        "    for line in text:        \n",
        "        temp_split = line.split(\"\\t\")\n",
        "        if len(temp_split) == 14:\n",
        "            if temp_split[3].lower() == search_word and temp_split[-1] == \"-\\n\":\n",
        "                temp_search = \"\\t{}\\t\".format(temp_split[3])\n",
        "                edited_text.append(line.replace(temp_search, replace_word))\n",
        "            else:\n",
        "                edited_text.append(line)\n",
        "        else:\n",
        "            edited_text.append(line)\n",
        "    return edited_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zypq8Csdx5-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a0df9a-b4ca-45eb-c762-0dbd63ed1710"
      },
      "source": [
        "edited_split_docs = []\n",
        "!mkdir edited_tdt\n",
        "files = [\"train.english.v4_gold_conll\", \n",
        "         \"dev.english.v4_gold_conll\",\n",
        "         \"test.english.v4_gold_conll\"]\n",
        "for split_file in files:\n",
        "    with open(split_file) as f:\n",
        "        text = f.readlines()\n",
        "    # A list of the pleonastic pronouns to be replaced\n",
        "    words = [\"it\"]\n",
        "    # Non-word to replace the pleonastic pronouns\n",
        "    replacement_word = \"XxXxX\"\n",
        "    temp = text\n",
        "    for word in words:\n",
        "        temp = word_replace(temp, word, replacement_word)\n",
        "    edited_split_docs.append(temp)\n",
        "    with open(\"edited_tdt/{}\".format(split_file), \"w\") as f:\n",
        "        f.writelines(temp) \n",
        "\n",
        "!zip -r edited_it_751510_11lines_wparts edited_tdt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: edited_tdt/ (stored 0%)\n",
            "  adding: edited_tdt/train.english.v4_gold_conll (deflated 89%)\n",
            "  adding: edited_tdt/dev.english.v4_gold_conll (deflated 89%)\n",
            "  adding: edited_tdt/test.english.v4_gold_conll (deflated 89%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgR7dpiayUzD",
        "outputId": "f53c0862-1488-46c9-bdf7-86c5eab80773"
      },
      "source": [
        "# Code to convert CoNLL to jsonlines format \n",
        "# This code as been adapted from minimize.py contained in https://github.com/mandarjoshi90/coref.git\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import tempfile\n",
        "import subprocess\n",
        "import collections\n",
        "\n",
        "import util\n",
        "import conll\n",
        "from bert import tokenization\n",
        "\n",
        "class DocumentState(object):\n",
        "  def __init__(self, key):\n",
        "    self.doc_key = key\n",
        "    self.sentence_end = []\n",
        "    self.token_end = []\n",
        "    self.tokens = []\n",
        "    self.subtokens = []\n",
        "    self.info = []\n",
        "    self.segments = []\n",
        "    self.subtoken_map = []\n",
        "    self.segment_subtoken_map = []\n",
        "    self.sentence_map = []\n",
        "    self.pronouns = []\n",
        "    self.clusters = collections.defaultdict(list)\n",
        "    self.coref_stacks = collections.defaultdict(list)\n",
        "    self.speakers = []\n",
        "    self.segment_info = []\n",
        "\n",
        "  def finalize(self):\n",
        "    # finalized: segments, segment_subtoken_map\n",
        "    # populate speakers from info\n",
        "    subtoken_idx = 0\n",
        "    for segment in self.segment_info:\n",
        "      speakers = []\n",
        "      for i, tok_info in enumerate(segment):\n",
        "        if tok_info is None and (i == 0 or i == len(segment) - 1):\n",
        "          speakers.append('[SPL]')\n",
        "        elif tok_info is None:\n",
        "          speakers.append(speakers[-1])\n",
        "        else:\n",
        "          speakers.append(tok_info[9])\n",
        "          if tok_info[4] == 'PRP':\n",
        "            self.pronouns.append(subtoken_idx)\n",
        "        subtoken_idx += 1\n",
        "      self.speakers += [speakers]\n",
        "    # populate sentence map\n",
        "\n",
        "    # populate clusters\n",
        "    first_subtoken_index = -1\n",
        "    for seg_idx, segment in enumerate(self.segment_info):\n",
        "      speakers = []\n",
        "      for i, tok_info in enumerate(segment):\n",
        "        first_subtoken_index += 1\n",
        "        coref = tok_info[-2] if tok_info is not None else '-'\n",
        "        if coref != \"-\":\n",
        "          last_subtoken_index = first_subtoken_index + tok_info[-1] - 1\n",
        "          for part in coref.split(\"|\"):\n",
        "            if part[0] == \"(\":\n",
        "              if part[-1] == \")\":\n",
        "                cluster_id = int(part[1:-1])\n",
        "                self.clusters[cluster_id].append((first_subtoken_index, last_subtoken_index))\n",
        "              else:\n",
        "                cluster_id = int(part[1:])\n",
        "                self.coref_stacks[cluster_id].append(first_subtoken_index)\n",
        "            else:\n",
        "              cluster_id = int(part[:-1])\n",
        "              try:\n",
        "                start = self.coref_stacks[cluster_id].pop()\n",
        "                self.clusters[cluster_id].append((start, last_subtoken_index))\n",
        "              except:\n",
        "                #print(\"Skipping {}\".format(seg_idx))  \n",
        "                pass\n",
        "    # merge clusters\n",
        "    merged_clusters = []\n",
        "    for c1 in self.clusters.values():\n",
        "      existing = None\n",
        "      for m in c1:\n",
        "        for c2 in merged_clusters:\n",
        "          if m in c2:\n",
        "            existing = c2\n",
        "            break\n",
        "        if existing is not None:\n",
        "          break\n",
        "      if existing is not None:\n",
        "        print(\"Merging clusters (shouldn't happen very often.)\")\n",
        "        existing.update(c1)\n",
        "      else:\n",
        "        merged_clusters.append(set(c1))\n",
        "    merged_clusters = [list(c) for c in merged_clusters]\n",
        "    all_mentions = util.flatten(merged_clusters)\n",
        "    sentence_map =  get_sentence_map(self.segments, self.sentence_end)\n",
        "    subtoken_map = util.flatten(self.segment_subtoken_map)\n",
        "    assert len(all_mentions) == len(set(all_mentions))\n",
        "    num_words =  len(util.flatten(self.segments))\n",
        "    assert num_words == len(util.flatten(self.speakers))\n",
        "    assert num_words == len(subtoken_map), (num_words, len(subtoken_map))\n",
        "    assert num_words == len(sentence_map), (num_words, len(sentence_map))\n",
        "    return {\n",
        "      \"doc_key\": self.doc_key,\n",
        "      \"sentences\": self.segments,\n",
        "      \"speakers\": self.speakers,\n",
        "      \"constituents\": [],\n",
        "      \"ner\": [],\n",
        "      \"clusters\": merged_clusters,\n",
        "      'sentence_map':sentence_map,\n",
        "      \"subtoken_map\": subtoken_map,\n",
        "      'pronouns': self.pronouns\n",
        "    }\n",
        "\n",
        "\n",
        "def normalize_word(word, language):\n",
        "  if language == \"arabic\":\n",
        "    word = word[:word.find(\"#\")]\n",
        "  if word == \"/.\" or word == \"/?\":\n",
        "    return word[1:]\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "# first try to satisfy constraints1, and if not possible, constraints2.\n",
        "def split_into_segments(document_state, max_segment_len, constraints1, constraints2):\n",
        "  current = 0\n",
        "  previous_token = 0\n",
        "  while current < len(document_state.subtokens):\n",
        "    end = min(current + max_segment_len - 1 - 2, len(document_state.subtokens) - 1)\n",
        "    while end >= current and not constraints1[end]:\n",
        "      end -= 1\n",
        "    if end < current:\n",
        "        end = min(current + max_segment_len - 1 - 2, len(document_state.subtokens) - 1)\n",
        "        while end >= current and not constraints2[end]:\n",
        "            end -= 1\n",
        "        if end < current:\n",
        "            raise Exception(\"Can't find valid segment\")\n",
        "    document_state.segments.append(['[CLS]'] + document_state.subtokens[current:end + 1] + ['[SEP]'])\n",
        "    subtoken_map = document_state.subtoken_map[current : end + 1]\n",
        "    document_state.segment_subtoken_map.append([previous_token] + subtoken_map + [subtoken_map[-1]])\n",
        "    info = document_state.info[current : end + 1]\n",
        "    document_state.segment_info.append([None] + info + [None])\n",
        "    current = end + 1\n",
        "    previous_token = subtoken_map[-1]\n",
        "\n",
        "def get_sentence_map(segments, sentence_end):\n",
        "  current = 0\n",
        "  sent_map = []\n",
        "  sent_end_idx = 0\n",
        "  assert len(sentence_end) == sum([len(s) -2 for s in segments])\n",
        "  for segment in segments:\n",
        "    sent_map.append(current)\n",
        "    for i in range(len(segment) - 2):\n",
        "      sent_map.append(current)\n",
        "      current += int(sentence_end[sent_end_idx])\n",
        "      sent_end_idx += 1\n",
        "    sent_map.append(current)\n",
        "  return sent_map\n",
        "\n",
        "def get_document(document_lines, tokenizer, language, segment_len):\n",
        "  document_state = DocumentState(document_lines[0])\n",
        "  word_idx = -1\n",
        "  for line in document_lines[1]:\n",
        "    row = line.split()\n",
        "    sentence_end = len(row) == 0\n",
        "    if not sentence_end:\n",
        "      assert len(row) >= 12\n",
        "      word_idx += 1\n",
        "      word = normalize_word(row[3], language)\n",
        "      subtokens = tokenizer.tokenize(word)\n",
        "      document_state.tokens.append(word)\n",
        "      document_state.token_end += ([False] * (len(subtokens) - 1)) + [True]\n",
        "      for sidx, subtoken in enumerate(subtokens):\n",
        "        document_state.subtokens.append(subtoken)\n",
        "        info = None if sidx != 0 else (row + [len(subtokens)])\n",
        "        document_state.info.append(info)\n",
        "        document_state.sentence_end.append(False)\n",
        "        document_state.subtoken_map.append(word_idx)\n",
        "    else:\n",
        "      document_state.sentence_end[-1] = True\n",
        "  # split_into_segments(document_state, segment_len, document_state.token_end)\n",
        "  # split_into_segments(document_state, segment_len, document_state.sentence_end)\n",
        "  constraints1 = document_state.sentence_end if language != 'arabic' else document_state.token_end\n",
        "  split_into_segments(document_state, segment_len, constraints1, document_state.token_end)\n",
        "  stats[\"max_sent_len_{}\".format(language)] = max(max([len(s) for s in document_state.segments]), stats[\"max_sent_len_{}\".format(language)])\n",
        "  document = document_state.finalize()\n",
        "  return document\n",
        "\n",
        "def skip(doc_key):\n",
        "  # if doc_key in ['nw/xinhua/00/chtb_0078_0', 'wb/eng/00/eng_0004_1']: #, 'nw/xinhua/01/chtb_0194_0', 'nw/xinhua/01/chtb_0157_0']:\n",
        "    # return True\n",
        "  return False\n",
        "\n",
        "def minimize_partition(name, language, extension, labels, stats, tokenizer, seg_len, input_dir, output_dir):\n",
        "  input_path = \"{}/{}.{}.{}\".format(input_dir, name, language, extension)\n",
        "  output_path = \"{}/{}.{}.{}.jsonlines\".format(output_dir, name, language, seg_len)\n",
        "  count = 0\n",
        "  print(\"Minimizing {}\".format(input_path))\n",
        "  documents = []\n",
        "  with open(input_path, \"r\") as input_file:\n",
        "    for line in input_file.readlines():\n",
        "      begin_document_match = re.match(conll.BEGIN_DOCUMENT_REGEX, line)\n",
        "      if begin_document_match:\n",
        "        doc_key = conll.get_doc_key(begin_document_match.group(1), begin_document_match.group(2))\n",
        "        documents.append((doc_key, []))\n",
        "      elif line.startswith(\"#end document\"):\n",
        "        continue\n",
        "      else:\n",
        "        documents[-1][1].append(line)\n",
        "  with open(output_path, \"w\") as output_file:\n",
        "    for document_lines in documents:\n",
        "      if skip(document_lines[0]):\n",
        "        continue\n",
        "      document = get_document(document_lines, tokenizer, language, seg_len)\n",
        "      output_file.write(json.dumps(document))\n",
        "      output_file.write(\"\\n\")\n",
        "      count += 1\n",
        "  print(\"Wrote {} documents to {}\".format(count, output_path))\n",
        "\n",
        "def minimize_language(language, labels, stats, vocab_file, seg_len, input_dir, output_dir, do_lower_case):\n",
        "  # do_lower_case = True if 'chinese' in vocab_file else False\n",
        "  tokenizer = tokenization.FullTokenizer(\n",
        "                vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "  minimize_partition(\"dev\", language, \"v4_gold_conll\", labels, stats, tokenizer, seg_len, input_dir, output_dir)\n",
        "  minimize_partition(\"train\", language, \"v4_gold_conll\", labels, stats, tokenizer, seg_len, input_dir, output_dir)\n",
        "  minimize_partition(\"test\", language, \"v4_gold_conll\", labels, stats, tokenizer, seg_len, input_dir, output_dir)\n",
        "\n",
        "\n",
        "vocab_file = \"cased_config_vocab/vocab.txt\"\n",
        "input_dir = \".\"\n",
        "output_dir = \".\"\n",
        "do_lower_case = 'true'\n",
        "print(do_lower_case)\n",
        "labels = collections.defaultdict(set)\n",
        "stats = collections.defaultdict(int)\n",
        "if not os.path.isdir(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "for seg_len in [128, 256, 384, 512]:\n",
        "    minimize_language(\"english\", labels, stats, vocab_file, seg_len, input_dir, output_dir, do_lower_case)\n",
        "    # minimize_language(\"chinese\", labels, stats, vocab_file, seg_len)\n",
        "    # minimize_language(\"es\", labels, stats, vocab_file, seg_len)\n",
        "    # minimize_language(\"arabic\", labels, stats, vocab_file, seg_len)\n",
        "for k, v in labels.items():\n",
        "    print(\"{} = [{}]\".format(k, \", \".join(\"\\\"{}\\\"\".format(label) for label in v)))\n",
        "for k, v in stats.items():\n",
        "    print(\"{} = {}\".format(k, v))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true\n",
            "Minimizing ./dev.english.v4_gold_conll\n",
            "Wrote 435 documents to ./dev.english.128.jsonlines\n",
            "Minimizing ./train.english.v4_gold_conll\n",
            "Wrote 2238 documents to ./train.english.128.jsonlines\n",
            "Minimizing ./test.english.v4_gold_conll\n",
            "Wrote 333 documents to ./test.english.128.jsonlines\n",
            "Minimizing ./dev.english.v4_gold_conll\n",
            "Wrote 435 documents to ./dev.english.256.jsonlines\n",
            "Minimizing ./train.english.v4_gold_conll\n",
            "Wrote 2238 documents to ./train.english.256.jsonlines\n",
            "Minimizing ./test.english.v4_gold_conll\n",
            "Wrote 333 documents to ./test.english.256.jsonlines\n",
            "Minimizing ./dev.english.v4_gold_conll\n",
            "Wrote 435 documents to ./dev.english.384.jsonlines\n",
            "Minimizing ./train.english.v4_gold_conll\n",
            "Wrote 2238 documents to ./train.english.384.jsonlines\n",
            "Minimizing ./test.english.v4_gold_conll\n",
            "Wrote 333 documents to ./test.english.384.jsonlines\n",
            "Minimizing ./dev.english.v4_gold_conll\n",
            "Wrote 435 documents to ./dev.english.512.jsonlines\n",
            "Minimizing ./train.english.v4_gold_conll\n",
            "Wrote 2238 documents to ./train.english.512.jsonlines\n",
            "Minimizing ./test.english.v4_gold_conll\n",
            "Wrote 333 documents to ./test.english.512.jsonlines\n",
            "max_sent_len_english = 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muOjyl7e7EBL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}